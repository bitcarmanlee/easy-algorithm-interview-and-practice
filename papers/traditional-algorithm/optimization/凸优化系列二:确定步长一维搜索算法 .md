## 1.精确一维搜索与非精确一维搜索
在上一篇文章中，我们提到第k次的迭代公式为:  
$$x_{k+1} = x_k + \alpha_kd_k$$  
其中，$\alpha_k$表示步长。接下来我们讨论一下怎么确定步长。  
我们令  
$$\varphi(\alpha_k) = f(x_k + \alpha_k d_k)$$  
假设我们从点$x_k$出发，沿着方向$d_k$进行搜索，确定$\varphi(\alpha_k)$值最小，这个过程就叫一维搜索。注意我们在这个搜索过程中假设$x_k$与$d_k$都已经确定，只有$\alpha_k$未知。  


如果能直接求出这个最优解$\alpha_k$，那么我们这个$\alpha_k$就被称为最优步长，这种方法被称为最优一维搜索，或者说精确一维搜索。
  
但是实际情况往往是问题比较复杂，数据维度也很高，直接求精确的最优步长$\alpha_k$可能比较困难，这个时候往往会选择不精确一维搜索来进行代替。  

不精确的一维搜索也可以成为近似一维搜索。通常的方法是选择合适的$\alpha_k$，使得目标函数有一定的下降量，即$f(x_k + \alpha_kd_k) < f(x_k)$。或者说，只需要找到一个步长，使得目标函数有一定的下降量就可以了。  


## 2.精确一维搜索之试探法  
精确一维搜索主要包括试探法(区间搜索法)与函数逼近法。  
其中，常用的试探法又包括进退法，黄金分割法，二分法等。  

### 2.1进退法
算法的步骤如下:  
1.确定搜索的起点与初始步长。  
2.以起点开始以初始步长向前试探。如果函数值变大，改变步长方向。  
3.如果函数值下降，维持原来的试探方向，并将步长加倍。  

算法的大致流程如下  
![在这里插入图片描述](https://github.com/bitcarmanlee/easy-algorithm-interview-photo/blob/master/traditional-algorithm/optimization/onedim_search/1.png)  
## 2.2 黄金分割法

0.618法，又叫黄金分割法，是优选法的一种。它在试验时，把试点安排在黄金分割点上来寻找最佳点。而生产生活中，我们常常取其近似值0.618，因此得名。0.618法是最常用的单因素单峰目标函数优选法之一。(参考文献1)  

用0.618法寻找最佳点时，虽然不能保证在有限次内准确找出最佳点，但随着试验次数的增加，最佳点被限定在越来越小的范围内，即存优范围会越来越小。用存优范围与原始范围的比值来衡量一种试验方法的效率，这个比值叫精度。用0.618法确定试点时，每一次实验都把存优范围缩小为原来的0.618.因此，n次试验后的精度为：  

$$\delta_n = 0.618^{n-1}$$  

具体的算法细节可以查阅更为详细的文献与参考资料。  

## 2.3 二分法
具体原理与黄金分割类似。  

## 3.精确一维搜索之函数逼近法  
如果原函数具有比较好的解析性质，那么可以使用函数逼近(插值)的方法。  
### 3.1 牛顿法  
牛顿法的思路就是利用某一点的函数值，一阶导数值，二阶导数值构造二次插值函数。牛顿法最大的优势就是收敛速度快，具有局部二阶收敛的速度。  

将$f(x)$在$x_k$点处泰勒展开  
$$f(x) = f(x_k) + f'(x_k)(x-x_k) + \frac{f''(x_k)}{2!}(x - x_k)^2 + o(x-x_k)^2$$  

要求上面函数的极值，由高等数学的知识，易知$f'(x) = 0$，那么有  
$$f'(x_k) + f''(x_k)(x - x_k) = 0$$  
求解可知  
$$x = x_k - \frac{f'(x_k)}{f''(x_k)}$$  

对应到一维搜索中，步长$\alpha$的迭代方式为:  
$$\alpha_{k+1} = \alpha_k - \frac{f'(\alpha_k)}{f''(\alpha_k)}$$  
每次更新该点，然后迭代查找即可。  

### 3.2 插值法
可以有相应的二次插值，三次插值方法，具体可以查看参考文献2关于插值方法的描述。  

## 4.不精确搜索  
由于实际问题的复杂性，使用精确一维搜索往往要付出很高的代价，还不一定能得到比较好的结果。后来慢慢发现，只要遵循一定的规律，算法就很可能达到收敛。  

### 4.1 Armijo-Goldstein准则  
Armijo-Goldstein准则的核心思想有两个:  
1.目标函数值应该有足够的下降  
2.一维搜索的步长$\alpha$不应该太小。  

这两个思想的意图非常明显。由于最优化问题的目的就是寻找极小值，因此，让目标函数函数值“下降”是我们努力的方向，所以1正是想要保证这一点。  
同理，2也类似：如果一维搜索的步长$\alpha$太小了，那么我们的搜索类似于在原地打转，可能也是在浪费时间和精力。(参考文献3)  


所以最后Armijo准则的表达式为两个式子:  
$$f(x_k + \alpha_k d_k) \le f(x_k) + \alpha_k \rho g_k^Td_k$$  
$$f(x_k + \alpha_k d_k) \ge f(x_k) + \alpha_k (1 - \rho) g_k^Td_k$$  
其中，$0 \lt \rho \lt 1/2$  

为什么上面两个式子就可以满足我们的要求，可以阅读参考文献3，或者查阅相关最优化理论的教材。  

### 4.2 Wolfe-Powell准则  
Armijo-Goldstein准则可能会把最优步长因子排除在可接受区间外，因此Wolfe-Powell准则做了相关的改进。  
Wolfe-Powell准则也是有两个表达式。第一个表达式与Armijo-Goldstein准则的第一个表达式相同，而第二个表达式为:  
$$\nabla f(x_k + \alpha_k d_k)^Td_k \ge \sigma g_k^T d_k, \quad \sigma \in (\rho, 1)$$  

上面式子的几何解释为：在可接受点处的切线斜率大于等于初始斜率的$\sigma$倍！  

参考文献：  
1.https://zh.wikipedia.org/wiki/黄金分割法  
2.https://blog.csdn.net/bitcarmanlee/article/details/86556744  
3.https://www.codelast.com/原创用人话解释不精确线搜索中的armijo-goldstein准则及wo/  