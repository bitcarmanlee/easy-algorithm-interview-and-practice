## 1.MSE损失函数
损失函数是机器学习与深度学习里面的重要概念。从名字上就可以看出，损失函数(Loss Function)反应的是模型对数据的拟合程度。一般来说，损失函数越小，说明模型对数据的拟合也越好。同时我们还希望当损失函数比较大的时候，对应的梯度也会比较大，这样梯度下降的时候更新也会快一些。  

线性回归中，最常用的就是最小平方误差(MSE)了。MSE也相当简单:  
$$Loss_{mse} = \sum _{i=1} ^n(y_i - \tilde{y}) ^ 2$$  
其中，$y_i$为样本的真实值，$\tilde{y}$为预测值。  
MSE的意义相当明确：如果预测值与真实值的欧式距离越大，损失函数越大。欧式距离越小，损失函数越小。同时，求导也是相当容易：  
$$\frac{\partial L}{\partial \theta} = 2 \sum _{i=1} ^n(y_i - \tilde{y}) \frac{\partial \tilde y}{\partial \theta} $$  
其中，$\theta$是模型中待训练的参数。  

一般来说，MSE是个很中庸的选择。用了MSE，一般不会有什么大毛病，但同时也不要指望他有特别优秀的表现。  

##  2.Sigmoid一般不与MSE配合使用  
上面求MSE导数的时候，注意到最后有一项是$\tilde y$对参数求导。在深度学习里，Sigmoid函数是常见的激活函数。特别注意的是，当使用Sigmoid做激活函数的时候，损失函数不能选择MSE。因为Sigmoid的导数为$f(x)(1-f(x))$。假设当预测值为$f(x)=1$而真实值为0的时候，此时虽然$(y_i - \tilde{y})$很大，但是$f(x)(1-f(x))$太小接近0，收敛速度同样很慢。  

## 3.常见的损失函数与激活函数的组合  
### 3.1 MSE + Sigmoid以外的激活函数  
第二部分特意提到，MSE一般不与Sigmoid函数配合使用。 
 
### 3.2 Cross Entropy + Sigmoid  
前面我们提到Sigmoid函数的导数为$f(x)(1-f(x))$。所以我们的损失函数最好是分母上有这一项。所以一般使用交叉熵作为损失函数:  
$$J(\theta)=cost(h_{\theta}(x),y) = -y_ilog(h_{\theta}(x)) - (1-y_i)log(1-h_{\theta}(x))$$  

### 3.3 Softmat + Cross Entropy / log-likehood  
对于多分类问题，Softmax是常用的方式。Softmax本质上是一个归一化变换，能将普通的输出变成一个概率输出。  

## 4.KL散度  
如果我们有另一个独立的随机变量x相关的事件B，该怎么计算它们之间的区别？  
此处我们介绍默认的计算方法：KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为KL散度不具备有对称性。在距离上的对称性指的是A到B的距离等于B到A的距离。  

举个不恰当的例子，事件A：张三今天买了2个土鸡蛋，事件B：李四今天买了6个土鸡蛋。我们定义随机变量x：买土鸡蛋，那么事件A和B的区别是什么？有人可能说，那就是李四多买了4个土鸡蛋？这个答案只能得50分，因为忘记了"坐标系"的问题。换句话说，对于张三来说，李四多买了4个土鸡蛋。对于李四来说，张三少买了4个土鸡蛋。选取的参照物不同，那么得到的结果也不同。更严谨的说，应该是说我们对于张三和李四买土鸡蛋的期望不同，可能张三天天买2个土鸡蛋，而李四可能因为孩子满月昨天才买了6个土鸡蛋，而平时从来不买。  


KL散度的数学定义：  

对于离散事件我们可以定义事件A和B的差别为  
$$D_{KL}(A||B) = \sum_i P_A(x_i)log\left(\frac{P_A(x_i)}{P_B(x_i)}\right) = \sum_i P_A(x_i)logP_A(x_i) - P_A(x_i)logP_B(x_i)$$  
对于连续事件，将求和改为积分即可。$$D_{KL}(A||B) =\int a(x) log\left(\frac{a(x)}{b(x)}\right)　$$  

由上面的公式可以看出:  
$$KL散度 = 交叉熵 - 熵$$  

## 5.交叉熵  
交叉熵的定义:  
$$H(A, B) = -\sum_{i=1}^n A(x_i)log(B(x_i)) $$  
参考上面的KL散度公式:  
$$D_{KL}(A||B) = \sum_i P_A(x_i)logP_A(x_i) - P_A(x_i)logP_B(x_i) = -H(p(x)) +  (-\sum_i P_A(x_i)logP_B(x_i))$$  
在机器学习或者深度学习中，经常需要做的就是评估label 与predict之间的差距，这个时候用KL散度就刚好符合要求，即$D_{KL}(y|| \tilde y)$。由于KL散度前面那部分的$-H(y)$是不变的，所以优化的时候，只需要关注交叉熵就可以了。因此，一般在算法中直接就是用交叉熵做loss用来评估模型的好坏。  



参考文献： 
1.https://www.zhihu.com/question/65288314  
2.https://blog.csdn.net/tsyccnh/article/details/79163834　一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉  